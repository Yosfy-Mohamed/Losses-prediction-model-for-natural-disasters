!pip install catboost
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
try:
  data = pd.read_csv('/content/drive/MyDrive/Project data/Datasets/Natural disasters dataset.csv', encoding='utf-8')
except UnicodeDecodeError:
  with open('/content/drive/MyDrive/Project data/Datasets/Natural disasters dataset.csv', encoding='utf-8' , errors='ignore') as f:
    data = pd.read_csv(f)


data.isnull().sum()
data = data.dropna(axis=0, subset=["Total Damage, Adjusted ('000 US$)"])


Q1 = data["Total Damage, Adjusted ('000 US$)"].quantile(0.25)
Q3 = data["Total Damage, Adjusted ('000 US$)"].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Show outliers (values outside the bounds)
outliers = data[(data["Total Damage, Adjusted ('000 US$)"] < lower_bound) | (data["Total Damage, Adjusted ('000 US$)"] > upper_bound)]


# Remove outliers
data_cleaned = data[(data["Total Damage, Adjusted ('000 US$)"] >= lower_bound) & (data["Total Damage, Adjusted ('000 US$)"] <= upper_bound)]

x = data_cleaned[['Start Year', 'Disaster Type' , 'Disaster Subtype' , "Total Damage ('000 US$)"]]
y = data_cleaned["Total Damage, Adjusted ('000 US$)"]

data_cleaned = data_cleaned.drop_duplicates()

categorical_features = ['Disaster Type', 'Disaster Subtype']
x[categorical_features] = x[categorical_features].astype('category')

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=42)

model = CatBoostRegressor(
    iterations=1000,          # Number of boosting rounds
    learning_rate=0.03,       # Smaller learning rate for smoother learning
    depth=7,                 # Depth of trees
    random_state=42,          # Reproducibility
    verbose=100,               # Print progress every 100 iterations
    early_stopping_rounds=50
)

param_dist = {
    'iterations': [500, 1000, 1500 , 2000],
    'learning_rate': [0.01, 0.03, 0.05],
    'depth': [6, 7],       # Controls tree depth
    'l2_leaf_reg': [10, 20, 50],  # L2 regularization
    'bagging_temperature': [0.5, 1, 2],  # Controls randomness
    'subsample': [0.8, 1.0]    # Row sampling
}
random_search = RandomizedSearchCV(
    estimator=CatBoostRegressor(verbose=0, random_state=42),
    param_distributions=param_dist,
    n_iter=10,  # Reduce iterations
    scoring='r2',
    cv=5,       # Reduce cross-validation folds
    verbose=2,
    random_state=42,
    n_jobs=-1
)

random_search.fit(x_train, y_train, cat_features=categorical_features)

# Get the best model
best_catboost = random_search.best_estimator_

# Make predictions
y_pred = best_catboost.predict(x_test)


# Graphs to be able to visualize difference between predicted and actual data
plt.scatter(y_test, y_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()
sns.pairplot(data_cleaned[['Disaster Type', 'Disaster Subtype', 'Total Damage, Adjusted (\'000 US$)']])


# Statistical analysis
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
std_dev = np.std(data_cleaned['Total Damage, Adjusted (\'000 US$)'])
correlation = data_cleaned[['Total Damage, Adjusted (\'000 US$)', 'No. Affected', 'Total Affected', "Total Damage ('000 US$)", 'Total Deaths']].corr()
print(correlation)



print("Mean absolute error:" , mae)
print("Mean squared error:" , mse)
print("R2 score:" , r2)
print('Standard Deviation:' , std_dev)


# Predictions using the best model from RandomizedSearchCV
y_train_pred = best_catboost.predict(x_train)
y_test_pred = best_catboost.predict(x_test)

# Training Performance
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

# Testing Performance
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Training MAE: {train_mae}, Testing MAE: {test_mae}")
print(f"Training MSE: {train_mse}, Testing MSE: {test_mse}")
print(f"Training R²: {train_r2}, Testing R²: {test_r2}")



while True:
    try:
        sy = int(input('Enter the start year:'))
        if sy in range(2023 , 2026):
            raise ValueError("The predicted losses will be the same as the total damage due to the year being too close to the current year/CPI will be 100.")
        dt = input("Enter the disaster type:")
        dst = input("Enter the disaster subtype:")
        td = float(input("Enter the total damage (in USD):"))
        break
    except ValueError as e:
        print(f"Invalid input: {e}")

# Create a DataFrame for user input
user_input = pd.DataFrame({
    'Start Year': [sy],
    'Disaster Type': [dt],
    'Disaster Subtype': [dst],
    "Total Damage ('000 US$)": [td]
})

# Convert categorical features to category type
user_input['Disaster Type'] = user_input['Disaster Type'].astype('category')
user_input['Disaster Subtype'] = user_input['Disaster Subtype'].astype('category')

# Predict using the trained CatBoost model
Prediction = best_catboost.predict(user_input)
print('Predicted adjusted damage in USD:', Prediction)
